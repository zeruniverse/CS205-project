<!DOCTYPE html>
<!-- saved from url=(0028)https://blog.getpelican.com/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<head>
        <title>Parallelized Deepflow</title>
        <link rel="stylesheet" href="main.css" type="text/css">

        <!--[if IE]>
            <script src="https://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="index.html">Parallelized DeepFlow</a></h1>
                <nav><ul>
                      <li><a href="index.html">Introduction</a></li>
                      <li><a href="design.html">Design</a></li>
                      <li class="active"><a href="performance.html">Performance</a></li>
                      <li><a href="applications.html">Applications</a></li>
                      <li><a href="conclusion.html">Conclusion</a></li>
                </ul></nav>
        </header><!-- /#banner -->
        
<section id="content" class="body">
    <h2 class='entry-title' id="data-platform">Evaluation Data & Platform</h2>

    <p>Any online video and any two consecutive images can be used as test data for our model. Please see our <a href="https://github.com/zeruniverse/CS205-project/blob/master/README.md#prepare-data">GitHub README</a> for details if you want to prepare your data with your own video.</p>

    <p>We evaluated our implementations with the sample video downloaded online (<a href="https://github.com/zeruniverse/CS205-project/releases/download/0.01/bunny.mp4">Bunny</a> and <a href="https://github.com/zeruniverse/CS205-project/releases/download/0.01/nyc1080.mp4">NYC</a>). The frames of videos (<a href="https://github.com/zeruniverse/CS205-project/releases/download/0.01/bunny_640x360_frame.zip">360p</a>, <a href="https://github.com/zeruniverse/CS205-project/releases/download/0.01/bunny_1280x720_first999_frame.zip">720p</a>, <a href="https://github.com/zeruniverse/CS205-project/releases/download/0.01/nyc_1920x1080_frame.zip">1080p</a>) can be extracted with <a href="https://en.wikipedia.org/wiki/FFmpeg">FFmpeg</a> and the pairwise matches (<a href="https://github.com/zeruniverse/CS205-project/releases/download/0.01/bunny_640x360_match.zip">360p</a>, <a href="https://github.com/zeruniverse/CS205-project/releases/download/0.01/bunny_1280x720_match.zip">720p</a>, <a href="https://github.com/zeruniverse/CS205-project/releases/download/0.01/nyc_1920x1080_match.zip">1080p</a>) can be generated using our <a href="https://github.com/zeruniverse/CS205-project/blob/master/tools/deepmatching-video.sh">helper script</a> (HPC-ready). We generated both forward and backward flows for videos since both <a href="applications.html">applications</a> we presented use them. Therefore, if we have N frames in a video, we need to generate (N-1)*2 flows. We use <a href="https://www.rc.fas.harvard.edu/resources/odyssey-quickstart-guide/">Odyssey</a> to generate matches.</p>

    <p>For evaluating DeepFlow between two images, we generate forward flow between first two frames of the above videos (so we have 360p, 720p and 1080p images). The command is:</p>

    <pre>time ./deepflow2 out/frame_000001.ppm out/frame_000002.ppm flow.flo -match match/forward_1_2.match</pre>

    <p>For evaluating DeepFlow for processing videos, except MapReduce, we process pair by pair serially using a <a href="https://github.com/zeruniverse/CS205-project/blob/master/src/Jacobi_serial/video_flow.sh">bash script</a>. The accumulated time will be outputed on terminal. Since we know <b>the time cost increase linearly as the video length increases by theory</b>, we only report the <b>average per second video processing time</b>. Since all our video are with 25fps, that's the time to generate 50 flows (For 1-second length video, it's actually 48 flows. But with a video long enough, that's 50 flows per second on average). For videos of 360p, we run for 101 frames (the accumulated time divided by 4 is the average per second time). For video with 720p and 1080p, we run for 26 frames (the accumulated time is the average per second time). The command is:</p>

    <pre>bash video_flow.sh</pre>

    <p>To make fair comparison, we use <b>AWS m4.2xlarge</b> for all experiments except OpenACC (where we use <b>AWS g3.4xlarge</b>). The reason we don't use Odyssey is that it doesn't provide some necessary libraries and we don't have privilege to install them. Except MapReduce (where we use <b>emr-5.13.0</b>), we use <b>Ubuntu Server 16.04 LTS (HVM)</b> (ami-4e79ed36) as the operating system. All of original implementation and our implementations uses <b>-O3</b> compile flag and default DeepFlow parameters.</p>

    <p>A more detailed guide is provided in our <a href="https://github.com/zeruniverse/CS205-project/blob/master/README.md#generate-flow">GitHub README</a> if you want to reproduce our test result.</p>

    <hr />
    <h2 class='entry-title' id="serial">Serial</h2>

    <p>The table below shows the processing time for a pair of images. "SOR Serial Optimized" is the original implementation. The optimization stands for serial optimization which reduces the number of calculations.</p>

    <table style="border: 1px solid black;text-align:center">
        <caption>Time for generating flow between two images</caption>
        <tr style="border: 1px solid black;"><th></th><th>360p Images</th><th>720p Images</th><th>1080p Images</th></tr>
        <tr><td>SOR Serial</td><td>6.494 seconds</td><td>26.997 seconds</td><td>59.326 seconds</td></tr>
        <tr><td><b>SOR Serial Optimized</b></td><td>4.598 seconds</td><td>19.382 seconds</td><td>42.582 seconds</td></tr>
        <tr><td>Jacobi Serial</td><td>3.870 seconds</td><td>16.574 seconds</td><td>35.765 seconds</td></tr>
        <tr><td>Red-Black SOR Serial</td><td>3.687 seconds</td><td>15.812 seconds</td><td>34.476 seconds</td></tr>
    </table>

    <p>From SOR to Jacobi or Red-Black SOR, we removed dependencies inside the big iteration (so iterations inside the big iteration becomes independent). With -O3 flag, compilers can probably optimize our code more. Therefore, it's not surprising that our Jacobi and Red-Black SOR is faster than even the optimized SOR. However, as discussed in the <a href="design.html#rbsor">design page</a>, the result of Jacobi is pretty different from the original implementation, which indicates Jacobi is probably not good to use. Further considering that Red-Black SOR is faster than Jacobi, we have totally no reason to use it. Before we came up with Red-Black SOR, <b>we implemented the OpenMP and OpenACC versions of Jacobi. However, we didn't evaluate them due to the reason stated</b>.</p>

    <p>We also shows the video result below:</p>

    <table style="border: 1px solid black;text-align:center">
        <caption>Average per-second video processing time (flow generation)</caption>
        <tr style="border: 1px solid black;"><th></th><th>360p Video</th><th>720p Video</th><th>1080p Video</th></tr>
        <tr><td>SOR Serial</td><td>323 seconds</td><td>1316 seconds</td><td>2950 seconds</td></tr>
        <tr><td><b>SOR Serial Optimized</b></td><td>226 seconds</td><td>926 seconds</td><td>2129 seconds</td></tr>
        <tr><td>Jacobi Serial</td><td>192 seconds</td><td>788 seconds</td><td>1791 seconds</td></tr>
        <tr><td>Red-Black SOR Serial</td><td>183 seconds</td><td>752 seconds</td><td>1725 seconds</td></tr>
    </table>

    <p>Just by replacing original SOR solver with Red-Black SOR, we achieved <b>1.235 speedup</b>. <b>All parallel implementations below are based on the serial version of Red-Black SOR, and we choose the serial Red-Black SOR result as the baseline</b>.</p>

    <hr />

    <h2 class='entry-title'>OpenMP</h2>

    <p>The following two tables show the OpenMP result.</p>

    <table style="border: 1px solid black;text-align:center">
        <caption>Time for generating flow between two images</caption>
        <tr style="border: 1px solid black;"><th></th><th>360p Images</th><th>720p Images</th><th>1080p Images</th></tr>
        <tr><td>1 thread (serial)</td><td>3.687 seconds</td><td>15.812 seconds</td><td>34.476 seconds</td></tr>
        <tr><td>2 threads</td><td>2.518 seconds</td><td>11.588 seconds</td><td>25.253 seconds</td></tr>
        <tr><td>4 threads</td><td>1.910 seconds</td><td>9.160 seconds</td><td>19.730 seconds</td></tr>
        <tr><td>8 threads</td><td>2.048 seconds</td><td>9.171 seconds</td><td>19.883 seconds</td></tr>
        <tr><td>8 threads on m4.4xlarge</td><td>1.698 seconds</td><td>7.959 seconds</td><td>17.174 seconds</td></tr>
    </table>

    <p>To our surprise, the 8-thread result is worse than the 4-thread result. We investigated this and found we only have 4 actual cores in m4.2xlarge nodes (2 threads per core due to hyper-threading). This means if we have 2 threads in a core, the 2 threads will both slow down due to resources sharing. To show the general trend, we also tested on m4.4xlarge nodes where 8 cores (16 threads) are available. The result is normal (slightly faster than 4-thread).</p>

    <table style="border: 1px solid black;text-align:center">
        <caption>Average per-second video processing time (flow generation)</caption>
        <tr style="border: 1px solid black;"><th></th><th>360p Video</th><th>720p Video</th><th>1080p Video</th></tr>
        <tr><td>1 thread (serial)</td><td>183 seconds</td><td>752 seconds</td><td>1725 seconds</td></tr>
        <tr><td>4 threads</td><td>102 seconds</td><td>414 seconds</td><td>1004 seconds</td></tr>
    </table>

    <p>(Since testing on video is really time consuming, we only test with OMP_NUM_THREADS=4 as this performs best on m4.2xlarge node for 2 images)</p>

    <p>The following plot shows the speedup for DeepFlow between <b>two images</b>. The left one is the speedup plot for m4.2xlarge node, the right one is replacing the 8-thread result by the 8-thread result on m4.4xlarge (taking out the hyper-threading factor). <b>The right plot should represent the general trend</b>.</p>

    <figure style="clear:both">
            <img src="img/omp_org.png" alt="omp-org" width="330" style='margin-left:5px'>
            <img src="img/omp_fix.png" alt="omp-fix" width="330" style='margin-left:5px'>
    </figure>

    <p>The following plot shows the improvement of OpenMP on video:</p>

    <figure style="clear:both">
            <img src="img/omp_video.png" alt="omp-video" width="660">
    </figure>

    <hr />
    <h2 class='entry-title'>MPI</h2>
    <table style="border: 1px solid black;text-align:center">
        <caption>Time for generating flow between two images</caption>
        <tr style="border: 1px solid black;"><th></th><th>360p Images</th><th>720p Images</th><th>1080p Images</th></tr>
        <tr><td>MPI with 3 threads</td><td>9.398 seconds</td><td>37.481 seconds</td><td>88.902 seconds</td></tr>
        <tr><td>MPI with 4 threads</td><td>7.654 seconds</td><td>30.553 seconds</td><td>73.249 seconds</td></tr>
        <tr><td>MPI with 5 threads</td><td>6.759 seconds</td><td>27.357 seconds</td><td>70.386 seconds</td></tr>
        <tr><td>MPI with 6 threads</td><td>6.182 seconds</td><td>27.802 seconds</td><td>62.604 seconds</td></tr>
        <tr><td>MPI with 7 threads</td><td>5.857 seconds</td><td>23.765 seconds</td><td>54.823 seconds</td></tr>
        <tr><td>MPI with 8 threads</td><td>5.577 seconds</td><td>23.662 seconds</td><td>53.392 seconds</td></tr>
        <tr><td>MPI with 16 threads</td><td>7.804 seconds</td><td>31.968 seconds</td><td>72.716 seconds</td></tr>
    </table>
    <p>We implement MPI and run our model on 2 instances, i.e 2 CPUs, and the maximum number of threads now is 16, since each m4.2xlarge provides 8 threads. But since m4.2xlarge only provides 4 actual core, the performance of MPI with 16 threads is worse than one with 8 threads, which agrees with the result of previous OpenMP.
    <p>Although performance increases with the number of threads, MPI still works really bad, even worse than serial programming, which is no surprise, since messages passing between nodes are huge and frequent due to frequent call of Red-Black SOR, so that overhead is pretty large. Notice that we are not dealing with a single matrix and perform Red-Black SOR on it. Instead, we are dealing with multiple matrices with different sizes, so that every time we call Red-Black SOR, we need to reassign jobs to different nodes and those nodes send and receive messages, and that is how overhead accumulates.
           
    <hr />
    <h2 class='entry-title'>MPI+OpenMP</h2>
    <table style="border: 1px solid black;text-align:center">
        <caption>Time for generating flow between two images</caption>
        <tr style="border: 1px solid black;"><th></th><th>360p Images</th><th>720p Images</th><th>1080p Images</th></tr>
        <tr><td>MPI with 3 threads+OpenMP with 4 threads</td><td>7.133 seconds</td><td>28.026 seconds</td><td>69.213 seconds</td></tr>
        <tr><td>MPI with 4 threads+OpenMP with 4 threads</td><td>6.077 seconds</td><td>26.488 seconds</td><td>67.775 seconds</td></tr>
        <tr><td>MPI with 5 threads+OpenMP with 4 threads</td><td>8.428 seconds</td><td>29.043 seconds</td><td>64.877 seconds</td></tr>
    </table>
    <p>We also implement MPI with OpenMP. As we can see, the performance is better than MPI's but still worse than serial. The reasons are the same as MPI case. We therefore conclude that MPI does not fit this problem.
    
    <hr />
    <h2 class='entry-title'>OpenACC</h2>
    <p>Since PGCC does not support v4sf type, we rewrote the code. However, this will make the serial part of our code slower.</p>

    <table style="border: 1px solid black;text-align:center">
        <caption>Time for generating flow between two images</caption>
        <tr style="border: 1px solid black;"><th></th><th>360p Images</th><th>720p Images</th><th>1080p Images</th></tr>
        <tr><td>OpenACC</td><td>3.737 seconds</td><td>16.499 seconds</td><td>28.385 seconds</td></tr>
    </table>

    <p>The CPU serial execution time is similar on g3.4xlarge node and m4.2xlarge node. We see compared to serial version, OpenACC is slower on 360p and 720p images and slightly faster on 1080p images. We think the reason is (1) the v4sf workaround mentioned above slows down the performance. (2) Our matrices are not big enough so the memory copy overhead plays an important role. This also explains why for 1080p images, OpenACC will have better performance.</p>

    <p>According to the two-image result, we conclude OpenACC is not a good choice for DeepFlow parallelization. So we didn't evaluate OpenACC on video tasks.</p>

    <hr />

    <h2 class='entry-title'>MapReduce + OpenMP</h2>

    <p><b>MapReduce can only be applied to video tasks</b></p>

    <p><b>Note: We called Amazon and they don't agree to increase our m4.2xlarge instances limit to 9. So the 8-node result is actually 4 m4.2xlarge instances and 4 r4.2xlarge</b>. r4.2xlarge is the most similar instance with m4.2xlarge we can find and it's slightly more powerful than m4.2xlarge.</p>

    <p>From the OpenMP result, we know 4 thread is the best. So we set OMP_NUM_THREADS=4 as environment when running the OpenMP implementation. As a further trick to speedup the execution, we assign a pair of images to each worker node and <b>run forward and backward flow simultaneously</b>. Thus using OpenMP implementation as baseline, the ideal speedup would be 2 * #Nodes</p>

    <table style="border: 1px solid black;text-align:center">
        <caption>Average per-second video processing time (flow generation)</caption>
        <tr style="border: 1px solid black;"><th></th><th>360p Video</th><th>720p Video</th><th>1080p Video</th></tr>
        <tr><td>Serial</td><td>183 seconds</td><td>752 seconds</td><td>1725 seconds</td></tr>
        <tr><td>OpenMP</td><td>102 seconds</td><td>414 seconds</td><td>1004 seconds</td></tr>
        <tr><td>2 nodes</td><td>159 seconds</td><td>280 seconds</td><td>556 seconds</td></tr>
        <tr><td>4 nodes</td><td>83 seconds</td><td>193 seconds</td><td>307 seconds</td></tr>
        <tr><td>8 nodes</td><td>48 seconds</td><td>112 seconds</td><td>147 seconds</td></tr>
    </table>

    <p>The following plot shows the speedup using serial version as the baseline.</p>

    <figure style="clear:both">
            <img src="img/mapred-speedup.png" alt="mapred-speedup" width="660">
    </figure>

    <p>As shown in the figure, the speedup increase is quite linear. Due to the issue we mentioned before (r4.2xlarge is slightly more powerful than m4.2xlarge), from 4 nodes to 8 nodes, the speedup grows faster (without the issue, we expect linear speedup increase).</p>

    <p>Since there are overheads like HDFS file copys in MapReduce, and those overheads are approximately constants. By theory, when more fraction of the time are spent in flow calculation, we will have a higher speedup. Therefore, we observe in the above plot that 360p video has the worst speedup while 1080p video has the best speedup.</p>

</section>
<footer id="contentinfo" class="body">
        <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a></p>
</footer><!-- /#contentinfo -->

</body></html>
